---
# Name of the organizer
name: Dr. Jae Hee Lee

# Link to the organizer's webpage
webpage: https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/lee.html

# Primary affiliation
affil: University of Hamburg
# Link to the primary affiliation
affil_link: https://uni-hamburg.de

# An image of the organizer (square aspect ratio works the best) (place in the `assets/img/organizers` directory)
img: jheelee.png

# Secondary affiliation
#affil2: BuzzFizz Corp
# Link to the secondary affiliation
#affil2_link: https://buzzfizz.corp

# ORCID ID
orcid: 0000-0001-9840-780X

# Google Scholar profile link
googlescholarlink: https://scholar.google.com/citations?user=QNTE6VMAAAAJ
---

<!-- Add your bio here -->

Jae Hee Lee is a postdoctoral researcher in the Knowledge Technology Group, University of
Hamburg, Germany. His research interest is in building robust multimodal language models that
generalize to new tasks while retaining previously learned knowledge, where XAI techniques are
used to improve robustness. To pursue this XAI research, he was awarded with a funding from
the [German Research Foundation (title: Lifelong Multimodal Language Learning by Explaining
and Exploiting Compositional Knowledge)](https://gepris.dfg.de/gepris/projekt/551629603?language=en), which starts mid-2025. In recent years, he has
given lectures on [transformers](https://jaeheelee.gitlab.io/transformers_2024.pdf) and [large language models (LLMs)](https://jaeheelee.gitlab.io/llms_2024.pdf) that include mechanistic
interpretability and has been leading a [reading group on XAI and
LLMs](https://jaeheelee.gitlab.io/index.html#rg).
Previously, he was a postdoc at Cardiff University,
the University of Technology Sydney, and the Australian National University.
He obtained his PhD and Diplom degree from the University of Bremen, Germany.
