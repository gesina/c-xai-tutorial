{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECAI hands‑on: Exploring Concept-based Explainable AI for Computer Vision\n",
        "\n",
        "**Authors:** [Mahdi Pourghasem](https://www.linkedin.com/in/mahdi-pourghasem), [Gesina Schwalbe](https://gesina.github.io), [Jae Hee Lee](https://jaeheelee.gitlab.io)\n",
        "\n",
        "**Goal:** 30–45 min hands‑on Colab that introduces feature attribution, feature visualization, unsupervised concept extraction, and a supervised concept-based explainability (TCAV‑style) experiment. All examples build on the PyTorch/torchvision deep learning framework.\n",
        "\n",
        "Exercises are provided as textual descriptions in in the code respectivel marked with `=== EXERCISE`.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Setup & quick model demo**  \n",
        "   *(You might need to restart the session after first call to the `!pip` cell!)*\n",
        "2. **Feature attribution (Grad‑CAM)**\n",
        "\n",
        "   * Run a classical feature importance method and inspect the heatmaps\n",
        "3. **Feature visualization (most‑activating images for a filter/neuron)**\n",
        "\n",
        "   * Collect and show images that maximally activate a chosen filter\n",
        "4. **Unsupervised concept extraction (PCA / NMF / k‑means on activations)**\n",
        "\n",
        "   * Extract linear components / clusters as prototypes and show most activating images\n",
        "5. **Supervised concept extraction (simple TCAV style using synthetic concept)**\n",
        "\n",
        "   * Create a synthetic concept dataset, train a linear concept classifier in activation space\n",
        "   \n",
        "6. **Evaluating Concept-to-output Attribution (TCAV)**\n",
        "\n",
        "   * Compute the TCAV score for different concept activation vectors.\n",
        "\n",
        "6. **Wrap up**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r1lJ2oS--Z4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this first cell in Colab\n",
        "!pip install -q torch torchvision matplotlib scikit-learn tqdm numpy pandas scikit-image"
      ],
      "metadata": {
        "id": "o57nguny_u0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: !! You might need to restart the session after the installation. !!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 0) Imports & utilities\n",
        "\n",
        "> Note: We use `torchvision.models.resnet18(pretrained=True)` and `CIFAR10` (resized) as a small dataset that downloads automatically in Colab. These can be exchanged for other models, but make sure to adapt the setup accordingly (transformation, chosen target layer)."
      ],
      "metadata": {
        "id": "PbqCS9Gg_u_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: imports & utilities\n",
        "import torch, torchvision, cv2, numpy as np, matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.decomposition import PCA, NMF\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm.auto import tqdm\n",
        "import random, os\n",
        "from copy import deepcopy\n",
        "import PIL.Image\n",
        "\n",
        "# Just for typing\n",
        "from PIL.Image import Image\n",
        "from typing import Union\n",
        "\n",
        "# Helper variables\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Helper functions\n",
        "def imshow(img: Union[torch.Tensor, list[torch.Tensor]], title=None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Plot image (or list of images) in a row with optional titles.\n",
        "\n",
        "    Plots an image provided as torch tensor with values in [0,1].\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(img) and len(img.size()) > 3: img = list(img)\n",
        "    imgs: list[torch.Tensor] = img if type(img) == list else [img]\n",
        "    titles: list = title if type(title) == list else [title]*len(imgs)\n",
        "    assert len(imgs) == len(titles)\n",
        "\n",
        "    fig = plt.figure(figsize=(4*len(imgs),4))\n",
        "    axes = fig.subplots(1,len(imgs), squeeze=False)\n",
        "    for ax, img, title in zip(axes[0], imgs, titles):\n",
        "      if torch.is_tensor(img):\n",
        "        img = img.cpu().numpy().transpose(1,2,0)\n",
        "        img = np.clip(img, 0, 1)\n",
        "      ax.imshow(img)\n",
        "      if title: ax.set_title(title)\n",
        "      ax.axis('off')\n",
        "    return fig\n",
        "\n",
        "\n",
        "def overlay_heatmap(img: np.ndarray, heatmap: np.ndarray, alpha=0.5,\n",
        "                    colormap=cv2.COLORMAP_JET) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Overlay a heatmap over an image.\n",
        "\n",
        "    The image should be a tensor, PIL image, or the numpy array of a PIL image.\n",
        "    The heatmap should be a numpy array with values in [0,1].\n",
        "    \"\"\"\n",
        "    img_np = np.array(to_pil_image(img) if torch.is_tensor(img) else img)\n",
        "    heatmap_np = np.array(to_pil_image(heatmap, mode='F')) if torch.is_tensor(heatmap) else heatmap\n",
        "    heatmap_np = cv2.resize(heatmap_np, (img_np.shape[0], img_np.shape[1]))\n",
        "    heatmap_np = np.uint8(255 * heatmap_np)\n",
        "    heatmap_np = cv2.applyColorMap(heatmap_np, colormap)\n",
        "    overlay = cv2.addWeighted(heatmap_np, alpha, img_np, 1-alpha, 0)\n",
        "    return overlay\n",
        "\n",
        "def mask_heatmap(img: np.ndarray, heatmap: np.ndarray, alpha=1.) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Mask an image according to a heatmap.\n",
        "    \"\"\"\n",
        "    #return overlay_heatmap(img, heatmap, alpha=alpha, colormap=cv2.COLORMAP_BONE)\n",
        "    img_np = np.array(to_pil_image(img) if torch.is_tensor(img) else img)\n",
        "    heatmap_np = np.array(to_pil_image(heatmap, mode='F')) if torch.is_tensor(heatmap) else heatmap\n",
        "    heatmap_np = cv2.resize(heatmap_np, (img_np.shape[0], img_np.shape[1]))\n",
        "    masked_np = np.uint8(np.expand_dims(heatmap_np, axis=-1) * img_np)\n",
        "    overlay = cv2.addWeighted(masked_np, alpha, img_np, 1-alpha, 0)\n",
        "    return overlay\n",
        "\n",
        "\n",
        "def undo_normalize_transform(orig_mean: tuple, orig_std: tuple):\n",
        "    \"\"\"\n",
        "    Create a transformation that undos a normalization.\n",
        "    (Needed for visualization of images from the transformed dataset.)\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Normalize(mean = [ 0., 0., 0. ], std = [1/std_i for std_i in orig_std]),\n",
        "        transforms.Normalize(mean = [ -mean_i for mean_i in orig_mean], std = [ 1., 1., 1. ]),\n",
        "        ])\n",
        "\n",
        "def numpy_sigmoid(z): return 1.0/(1.0 + np.exp(-z))"
      ],
      "metadata": {
        "id": "wm09FsSz-nJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(If you got an error in the previous step after the first-time run of the `!pip` command, make sure to restart the notebook!)*\n",
        "\n",
        "## 1) Dataset & model setup"
      ],
      "metadata": {
        "id": "8RqC09h9-acg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: dataset and model\n",
        "\n",
        "# Choose and load the models\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "# Alternatives (check out https://docs.pytorch.org/vision/stable/models.html#classification):\n",
        "#alexnet, AlexNet_Weights\n",
        "#VGG16_BN_Weights\n",
        "\n",
        "# Load pretrained weights and initialize model for inference\n",
        "weights = ResNet18_Weights.DEFAULT  # ResNet18 predicting 1000 classes of ImageNet\n",
        "model = resnet18(weights=weights).to(device).eval()\n",
        "\n",
        "\n",
        "# Chose the right transformation for the model ...\n",
        "transform = weights.transforms()\n",
        "unnormalize = undo_normalize_transform(transform.mean, transform.std) # for plotting\n",
        "# ... and initialize the training and test data as well as the data loader\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "classes = trainset.classes\n",
        "\n",
        "# (small subsets for speed in the tutorial)\n",
        "train_subset = Subset(trainset, list(range(2000)))\n",
        "test_subset = Subset(testset, list(range(1000)))\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "print(f'Device: {device}')\n",
        "print(f'Model loaded; data ready with '\n",
        "      f'{len(train_subset)} train and {len(test_subset)} test samples.')"
      ],
      "metadata": {
        "id": "CbJaFnCK-sce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** run a forward pass on one batch and print the top‑3 predicted ImageNet label indices (we won't map them to names in this short tutorial).\n"
      ],
      "metadata": {
        "id": "uNuffxpk-aga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # <<<==== EXERCISE: Uncomment to explore\n",
        "# # Load and show an example image\n",
        "# img, label = train_subset[0]\n",
        "# imshow(unnormalize(img), title=f\"{classes[label]} ({label=})\");\n",
        "\n",
        "# # Conduct inference on the image\n",
        "# out = model(img.unsqueeze(0).to(device))\n",
        "# print(f\"Output dimensions: {out.size()}\")\n",
        "\n",
        "# # Retrieve top 3 values\n",
        "# topk_vals, topk_indices = torch.topk(out, k=3, dim=-1)\n",
        "# display(topk_vals)\n",
        "# display(topk_indices)"
      ],
      "metadata": {
        "id": "Cc4l1g-RqelC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Let's first have a closer look at the module structure of our model by printing it. What is the name of the last convolutional layer?"
      ],
      "metadata": {
        "id": "8pNbr6PPzxUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # <<<==== EXERCISE: Uncomment to explore\n",
        "# print(model)\n",
        "\n",
        "# # Names of submodules are hierarchical, separated by a dot\n",
        "# # (but mind that their order does not have to be the processing order)\n",
        "# print([name for name, module in model.named_modules()])"
      ],
      "metadata": {
        "id": "w3ejQjRIz3BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Feature attribution — Grad‑CAM\n",
        "\n",
        "We'll compute Grad‑CAM for a single convolutional layer (the last conv layer in ResNet18) and visualize heatmaps.\n",
        "\n",
        "*What to see here?* Grad-CAM is a feature attribution method. Feature attribution highlights which parts of the input were important for the output. In that these methods may mix up whatever features are availabe. C-XAI further drills down which different features the model has learned. Furthermore, we will need the gradient backpropagation techniques employed here later in a similar fashion for the calculation of the TCAV score.\n",
        "\n",
        "*How does it work?* Grad-CAM (1) collects the activation maps (convolutional intermediate outputs) of filters in a layer, (2) weights them according to their gradient with respect to the desired output, and (3) returns the weighted sum as heatmap.\n",
        "We here do a minimal implementation of the Grad-CAM algorithm."
      ],
      "metadata": {
        "id": "1VbUL7-a-aj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement a simple version of Grad-CAM."
      ],
      "metadata": {
        "id": "lns4yqvE5YxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Grad-CAM helpers\n",
        "last_conv_layer_name = 'layer4.1.conv2'  # ADAPT for different model\n",
        "\n",
        "\n",
        "# Activations and gradients get stored in these global variables.\n",
        "activations = None\n",
        "gradients = None\n",
        "\n",
        "# To store the activations (without gradient), we use the pytorch hook mechanism.\n",
        "# (1) define the hook functions:\n",
        "def save_activation(module, input, output):\n",
        "    global activations\n",
        "    activations = output.detach()\n",
        "\n",
        "def save_gradient(module, grad_input, grad_output):\n",
        "    global gradients\n",
        "    gradients = grad_output[0].detach()\n",
        "\n",
        "# (2) register the hook on last convolutional layer\n",
        "target_module = dict(model.named_modules())[last_conv_layer_name]\n",
        "\n",
        "handle_a = target_module.register_forward_hook(save_activation)\n",
        "handle_g = target_module.register_full_backward_hook(save_gradient)\n",
        "\n",
        "def grad_cam(input_tensor, model=model, target_class=None, device=device):\n",
        "    \"\"\"\n",
        "    Simple Grad-CAM implementation for classifiers.\n",
        "\n",
        "    Assumes activations and gradients are stored via hooks into the variables.\n",
        "    input_tensor must be of size [Batch, Channel, Height, Width].\n",
        "    \"\"\"\n",
        "    global activations, gradients\n",
        "    assert len(input_tensor.size()) == 4, f\"Invalid input tensor shape {input_tensor.size()}\"\n",
        "\n",
        "    model.train().zero_grad()  # start with zero gradients to not accumulate\n",
        "    # forward pass (collect activations)\n",
        "    out = model.to(device)(input_tensor.to(device))\n",
        "    if target_class is None:\n",
        "        # choose top predicted class\n",
        "        target_class = out.argmax(dim=1).item()\n",
        "    score = out[0, target_class]\n",
        "    # backward pass (collect gradients)\n",
        "    score.backward()\n",
        "\n",
        "    # Average the weights to obtain one weight per activation map channel\n",
        "    # dimensions: activations: [Batch, Channel, Height, Width], gradients: [B, C, H, W]\n",
        "    weights = gradients.mean(dim=(2,3), keepdim=True)  # [B, C, 1, 1]\n",
        "    # Weighted sum of activation channels (along channel dimension)\n",
        "    cam = (weights * activations)       # [B, C, H, W]\n",
        "    cam = cam.sum(dim=1)                # [B,    H, W]\n",
        "    # Clip to positive values\n",
        "    cam = F.relu(cam)\n",
        "\n",
        "    # Normalize to values in [0,1]\n",
        "    cam = cam - cam.min()\n",
        "    cam = cam / (cam.max() + 1e-8)\n",
        "\n",
        "    return cam.cpu().numpy()"
      ],
      "metadata": {
        "id": "aPQAUgsG-Wow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: example visualization for one image\n",
        "img, label = test_subset[0]  # <<====== EXERCISE: test different images\n",
        "cam = grad_cam(img.unsqueeze(0), model=model).squeeze(0)\n",
        "\n",
        "overlay = overlay_heatmap(to_pil_image(unnormalize(img)), cam)\n",
        "imshow([unnormalize(img), cam, overlay],\n",
        "       [f'input (class={classes[label]})', 'Grad-CAM heatmap', 'Grad-CAM overlay']);\n"
      ],
      "metadata": {
        "id": "htHD-nYP_EYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise (5 min):**\n",
        "\n",
        "* Try with several images/classes and observe where Grad‑CAM highlights.\n",
        "* Ask: what does this tell you about the model's focus? What is missing from such explanations?\n"
      ],
      "metadata": {
        "id": "yFRpbc-6_EOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Feature visualization — Most‑activating images for a neuron/filter\n",
        "\n",
        "We pick a single channel in the last convolutional layer activation map and find the images from the test subset that maximize the spatial mean of that channel.\n",
        "\n",
        "*For details see the [NetDissect paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.pdf)*."
      ],
      "metadata": {
        "id": "FBsJqXcE_D8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: compute activations for test dataset\n",
        "def collect_activations(loader, layer_module, device=device):\n",
        "    act_list = []\n",
        "    imgs_list = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in tqdm(loader, desc=\"Activations collected for batches:\"):\n",
        "            x = x.to(device)\n",
        "            _ = model(x)\n",
        "            a = activations.clone().cpu()  # dimensions: [B, C, H, W]\n",
        "            act_list.append(a)\n",
        "            imgs_list.append(x.cpu())\n",
        "    act_all = torch.cat(act_list, dim=0)\n",
        "    imgs_all = torch.cat(imgs_list, dim=0)\n",
        "    return act_all, imgs_all\n",
        "\n",
        "acts_all, imgs_all = collect_activations(test_loader, target_module)"
      ],
      "metadata": {
        "id": "V7CVmdHMGT1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: determine and show top k images per filter\n",
        "\n",
        "# Choose channel\n",
        "channel = 0  ## <<<======== EXERCISE: Try different channels\n",
        "k = 6        ## <<<======== EXERCISE: Try different k\n",
        "\n",
        "\n",
        "# Use the channel's mean activation as score per image\n",
        "scores = acts_all[:, channel].mean(dim=(1,2))        # dim: [B]\n",
        "\n",
        "# Helper function for top-k selection and plotting\n",
        "def show_topk(imgs: torch.Tensor, concept_scores: Union[np.ndarray, torch.Tensor],\n",
        "              k=k, suptitle=None):\n",
        "    \"\"\"\n",
        "    Select and show the top k scoring images from imgs.\n",
        "\n",
        "    imgs should be a Collection of image tensors,\n",
        "    the concept scores a flat tensor or numpy array of per-image scores.\n",
        "    \"\"\"\n",
        "    # Get indices of k top-scoring images\n",
        "    k = min(k, len(concept_scores))\n",
        "    topk_indices = torch.topk(torch.tensor(concept_scores), k=k).indices\n",
        "\n",
        "    # Plot\n",
        "    topk_imgs   = [unnormalize(imgs[idx]) for idx in topk_indices]\n",
        "    topk_titles = [f'score={concept_scores[idx]:.2f}, idx={int(idx)}'\n",
        "                   for idx in topk_indices]\n",
        "    fig = imshow(topk_imgs, topk_titles)\n",
        "    if suptitle:\n",
        "      fig.suptitle(suptitle)\n",
        "\n",
        "show_topk(imgs_all, scores, k=k,\n",
        "          suptitle=f'Most activating images for channel {channel}')"
      ],
      "metadata": {
        "id": "13_HELBB_EHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise:** Pick different channels. Do the top images show a coherent concept? Which channels are 'human interpretable'?\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Unsupervised concept extraction — PCA / NMF / k‑means\n",
        "\n",
        "We compute activation vectors for many probing images (for efficiency reasons spatially pooled). Then we can try to extract prototypical vectors via Principal Component Analysis (PCA) / Non-negative Matrix Factorization (NMF) / k-means clustering. To compare and explore, the images that most strongly project on each component / cluster centroid are visualized.\n",
        "\n",
        "> Note: Due to the spatial pooling, our prototypes have the shape $[C]$ of an activation map pixel. This comes in handy, because we can then either do the prototype comparison against again pooled activations (=per image scores for each concept), or compare activation map pixels (=per activation map pixel scores, resulting in a heatmap as in Grad-CAM). The latter can also be visualized by using the heatmap as a mask.\n",
        "\n",
        "*For details see the [ICE paper](https://ojs.aaai.org/index.php/AAAI/article/view/17389).*\n"
      ],
      "metadata": {
        "id": "_ffuRhds_Dnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: unsupervised concept mining with polysemantic neurons\n",
        "\n",
        "num_concepts = 6  # <<<== EXERCISE: Use different number of concepts\n",
        "k = 5\n",
        "\n",
        "# We here reduce an activation channel to a single value,\n",
        "# such that each image is represented by a vector of shape [1, C].\n",
        "pooled = acts_all.mean(dim=(2,3))  # shape [B, C]\n",
        "##                      ^^^== EXERCISE: Remove the reduction to adopt to concept segmentation\n",
        "pooled_np = pooled.numpy()\n",
        "\n",
        "\n",
        "# PCA: each principal component is a concept\n",
        "pca = PCA(n_components=num_concepts)  # obtain principal components (PC)\n",
        "pcs = pca.fit_transform(pooled_np) # project to PCs, dim: [B, num_concepts]\n",
        "# for each PC, find images with largest absolute projection\n",
        "for i in range(num_concepts):\n",
        "    concept_scores = pcs[:, i]\n",
        "\n",
        "    show_topk(imgs_all, concept_scores, suptitle=f'Principal Component {i} (top {k} samples)')\n",
        "\n",
        "\n",
        "# # <<<== EXERCISE: Try out alternative techniques for concept prototype mining\n",
        "# # NMF: find non-negative matrices U (projection), W (dictionary) such that\n",
        "# #      g(X)=pooled_np ~= U*W\n",
        "# # each dictionary entry in W is a concept vector.\n",
        "# nmf = NMF(n_components=num_concepts, random_state=0, max_iter=1000)\n",
        "# # !! the fitting requires only non-negative entries in X !!\n",
        "# S = nmf.fit_transform(pooled_np * (pooled_np>0)) # project to concept scores, dim: [B, num_concepts]\n",
        "# P = nmf.components_            # dictionary matrix, dim: [num_concepts, C]\n",
        "# for i in range(num_concepts):\n",
        "#     concept_scores = S[:, i]\n",
        "#     show_topk(imgs_all, concept_scores, suptitle=f'Dictionary entry {i} (top {k} samples)')\n",
        "\n",
        "\n",
        "# # <<<== EXERCISE: Try out alternative techniques for concept prototype mining\n",
        "# # k-means: each cluster is a concept\n",
        "# km = KMeans(n_clusters=num_concepts, random_state=0).fit(pooled_np)\n",
        "# concept_labels = km.labels_\n",
        "# for c in range(num_concepts):\n",
        "#     # subselect images classified as the concept\n",
        "#     idxs = np.where(concept_labels==c)[0]\n",
        "#     # find images closest to centroid\n",
        "#     centroid = km.cluster_centers_[c]\n",
        "#     dists = np.linalg.norm(pooled_np[idxs] - centroid, axis=1)\n",
        "\n",
        "#     show_topk(imgs_all[idxs], dists, k=k, suptitle=f'kmeans cluster {c} ({k} closest images)')"
      ],
      "metadata": {
        "id": "UJPtZpcI_Dv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can also be extended to segmentation (optional here)."
      ],
      "metadata": {
        "id": "C4cK0htpWOGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## OPTIONAL: EXTEND TO UNSUPERVISED CONCEPT SEGMENTATION ON CNNs\n",
        "\n",
        "# num_concepts = 6\n",
        "# k = 5\n",
        "\n",
        "# # >>> Comment in to re-activate pooling\n",
        "# #pooled = acts_all.mean(dim=(2,3))  # shape [B, C]\n",
        "# #pooled_np = pooled.numpy()\n",
        "# acts_np = acts_all[:100].numpy()   # shape [100, C, H, W] (subselect for efficiency)\n",
        "# B, C, H, W = acts_np.shape\n",
        "\n",
        "# # Helper function for top-k selection and plotting\n",
        "# def segment_topk(imgs: torch.Tensor, scores: Union[np.ndarray, torch.Tensor],\n",
        "#                  k=k, suptitle=None):\n",
        "#     \"\"\"\n",
        "#     Select and segment the top k scoring images from imgs.\n",
        "\n",
        "#     imgs should be a Collection of image tensors,\n",
        "#     the concept scores a tensor or numpy array of dim [B, H, W].\n",
        "#     \"\"\"\n",
        "#     k = min(k, len(concept_scores))\n",
        "#     heatmaps = torch.tensor(scores)  # dim [B, H, W] or [B]\n",
        "\n",
        "#     # Get per-image scores\n",
        "#     do_segment = heatmaps.dim() == 3\n",
        "#     scores = heatmaps.mean(dim=(1,2)) if do_segment else heatmaps\n",
        "\n",
        "#     # Get indices of k top-scoring images\n",
        "#     topk_indices = torch.topk(scores, k=k).indices\n",
        "\n",
        "#     # Plot\n",
        "#     topk_imgs   = [unnormalize(imgs[idx]) for idx in topk_indices]\n",
        "#     topk_titles = [f'score={scores[idx]:.2f}, idx={int(idx)}'\n",
        "#                    for idx in topk_indices]\n",
        "#     # Do per-image masking\n",
        "#     if do_segment:\n",
        "#       topk_imgs = [mask_heatmap(topk_img, (heatmaps[idx]>0.5)*1.0, alpha=1)\n",
        "#                    for idx, topk_img in zip(topk_indices, topk_imgs)]\n",
        "\n",
        "#     fig = imshow(topk_imgs, topk_titles)\n",
        "\n",
        "#     if suptitle:\n",
        "#       fig.suptitle(suptitle)\n",
        "\n",
        "# # PCA: each principal component is a concept\n",
        "# pca = PCA(n_components=num_concepts)  # obtain principal components (PC)\n",
        "\n",
        "# # >>> Comment in to reactivate pooling\n",
        "# #pca.fit(pooled_np)  # fitting could be done on the pooled version for efficiency\n",
        "# pca.fit(acts_np.reshape((-1, C)))\n",
        "\n",
        "# pcs = pca.transform(acts_np.reshape((-1, C)))  # project [B*H*W, C] -> [B*H*W, num_concepts]\n",
        "# # for each PC, find images with largest absolute projection\n",
        "# for i in range(num_concepts):\n",
        "#     concept_scores = pcs[:, i]\n",
        "#     concept_scores = pcs[:, i].reshape((B, H, W))\n",
        "#     # normalize\n",
        "#     concept_scores = numpy_sigmoid(concept_scores)\n",
        "#     segment_topk(imgs_all, concept_scores, suptitle=f'Principal Component {i} (top {k} positive projections)')\n",
        "\n",
        "\n",
        "# # k-means: each cluster is a concept\n",
        "# km = KMeans(n_clusters=num_concepts, random_state=0)\n",
        "\n",
        "# # >>> Comment in to reactivate pooling\n",
        "# #km.fit(pooled_np)  # fitting could be done on the pooled version for efficiency\n",
        "# km.fit(acts_np.reshape((-1, C)))\n",
        "# concept_labels = km.labels_\n",
        "# for c in range(num_concepts):\n",
        "#     concept_acts = acts_np.reshape((-1, C))\n",
        "\n",
        "#     # find images closest to centroid\n",
        "#     centroid = km.cluster_centers_[c]\n",
        "#     dists = np.linalg.norm(concept_acts - centroid, axis=1)\n",
        "\n",
        "#     dists = dists.reshape((-1, H, W))\n",
        "#     # normalize\n",
        "#     dists = (dists - dists.min())/(dists.max() - dists.min() + 0.000001)\n",
        "#     segment_topk(imgs_all, dists, k=k, suptitle=f'kmeans cluster {c} ({k} closest images)')"
      ],
      "metadata": {
        "id": "FjvQu7fRWO0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise (3 min):** compare PCA vs k‑means prototypes. Which yields more semantically coherent concepts? Discuss strengths/weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Supervised concept extraction — simplified CAV\n",
        "\n",
        "To keep the tutorial self‑contained we create a *synthetic concept* by inserting a small colored patch into images. That way participants can quickly label concept vs random and run a TCAV‑style pipeline.\n",
        "\n",
        "**Steps implemented below:**\n",
        "\n",
        "1. Create concept images by copying some test images and stamping a small red square in the corner.\n",
        "2. Compute activations for concept and random sets.\n",
        "3. Train a linear classifier on activations to get a concept direction (CAV).\n",
        "\n",
        "*For details see the [TCAV paper](http://proceedings.mlr.press/v80/kim18d.html)*."
      ],
      "metadata": {
        "id": "iun1-RGq_etg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: create synthetic concept (red patch) and showcase positive and negative samples\n",
        "num_pos_images = 200\n",
        "\n",
        "def add_red_patch(img_tensor, patch_size=30, pos=(0,0)):\n",
        "    x = img_tensor.clone()\n",
        "    x[:, pos[1]:pos[1]+patch_size, pos[0]:pos[0]+patch_size] = torch.tensor([1.0,-1.0,-1.0]).view(3,1,1)\n",
        "    return x\n",
        "\n",
        "# Build concept data:\n",
        "# (+) positive samples (take first 200 images and add patch)\n",
        "concept_idx = list(range(num_pos_images))\n",
        "concept_imgs = imgs_all[concept_idx].clone()\n",
        "for i in range(len(concept_imgs)):\n",
        "    concept_imgs[i] = add_red_patch(concept_imgs[i], patch_size=30, pos=(10,10))\n",
        "\n",
        "# (-) negative samples (200 random other images)\n",
        "random_idx = list(range(200,400))\n",
        "random_imgs = imgs_all[random_idx].clone()\n",
        "\n",
        "imshow([unnormalize(img) for img in concept_imgs[:3]], 'concept positive');\n",
        "imshow([unnormalize(img) for img in random_imgs[:3]], 'concept negative');"
      ],
      "metadata": {
        "id": "JNY5k7tVf4ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: collect activations for synthetic concept dataset\n",
        "\n",
        "# function to get pooled activations for a set of images\n",
        "@torch.no_grad()\n",
        "def pooled_activations_for_images(img_tensor_batch, batch_size=32):\n",
        "    pooled_all = []\n",
        "    model.eval()\n",
        "    for i in tqdm(range(0, len(img_tensor_batch), batch_size), \"Batch\"):\n",
        "        x = img_tensor_batch[i : (i+1)].to(device)\n",
        "        _ = model(x)\n",
        "        a = activations.clone().cpu().mean(dim=(2,3))\n",
        "        pooled_all.append(a)\n",
        "    return torch.cat(pooled_all, dim=0).numpy()\n",
        "\n",
        "# !! to reduce computational cost !! => used pooled activations\n",
        "concept_acts = pooled_activations_for_images(concept_imgs)\n",
        "random_acts = pooled_activations_for_images(random_imgs)"
      ],
      "metadata": {
        "id": "OgLMrQllgM1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise :**\n",
        "\n",
        "* Try different patch colors/positions and sizes, and see how TCAV score changes.\n",
        "* (Optional) Try using a real concept. E.g., pick sample images with label $1$ (='automobile' for CIFAR10) as positive class and random others as negative. What practical challenges arise?"
      ],
      "metadata": {
        "id": "c0E1X0mI_US0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------\n",
        "\n",
        "We can now calculate train the CAV.\n",
        "\n",
        "The CAV is the weight vector of a linear classifier that separates between the activations of positive and of negative concept samples. It represents the latent vector pointing into the direction of our concept.\n",
        "\n",
        "> Note: To reduce computational costs, we\n",
        "> 1. use pooled activations (CAV of dimensionality $[Channels]$) instead of the full activation map (spatially-aware CAV of dimensionality $[Channels, Height, Width]$);\n",
        "> 2. and use a logistic regressor instead of a linear support vector machine (try out the difference)."
      ],
      "metadata": {
        "id": "r-DyR1fnyFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: train linear concept classifier (concept model)\n",
        "\n",
        "X = np.vstack([concept_acts, random_acts])\n",
        "y = np.array([1]*len(concept_acts) + [0]*len(random_acts))\n",
        "concept_model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "\n",
        "# The model weights are the concept activation vector (CAV)\n",
        "cav = concept_model.coef_.reshape(-1)  # direction vector"
      ],
      "metadata": {
        "id": "YAHzedt83zbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Evaluating Concept-to-output Attribution — TCAV\n",
        "\n",
        "Now that we have collected several types of concept vectors, we can use these to inspect the processing of the model.\n",
        "One simple insight is whether the concept's presence has a tendency to positively or negatively contribute to an output class. This can be measured by the TCAV score, which completes the pipeline started above from the [TCAV paper](http://proceedings.mlr.press/v80/kim18d.html):\n",
        "\n",
        "4. Compute directional derivative of model output for a target class w.r.t. activations, projected onto the CAV, then compute TCAV score as fraction of images with positive directional derivative.\n",
        "\n",
        "\n",
        "*More details:* The TCAV score of a CAV requires to calculate the directional derivative of our (pooled) activation map along the CAV. The meaning of this derivative is: If it is positive, the CAV positively contributed to the output; and vice versa. Mind that this could be realized also using other feature attribution methods: One has to obtain an estimate of each activation channel's importance to the output, and can then project this to the CAV subspace using dot product.\n",
        "Counting the number of images from our test data with positive concept-to-output attribution yields the TCAV score of the CAV:\n",
        "$$ TCAV = \\frac{\\left|\\{x \\in X_{\\text{test}}, \\frac{\\partial f_{\\text{act}\\to\\text{out}}}{\\partial \\text{act}} \\cdot \\text{CAV}>0 \\}\\right|}{|X_{\\text{test}}|} $$\n",
        "Note that this evaluation can be done irrespective of where the CAV originates from. We can, thus, also compare our supervised CAV(s) with the unsupervised ones.\n",
        "\n",
        "*For details see the [TCAV paper](http://proceedings.mlr.press/v80/kim18d.html)*."
      ],
      "metadata": {
        "id": "CBkPpueE4Fb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: train CAV and calculate TCAV score\n",
        "\n",
        "# Set the CAV to be measured; dimension (due to pooling): [C]\n",
        "#cav_tensor: torch.Tensor = torch.tensor(cav)  # supervised CAV\n",
        "cav_tensor = torch.zeros(cav.shape); cav_tensor[channel] = 1. # unit vector (single filter)\n",
        "#cav_tensor = pca.components_[-1]  # principal component (numpy array)\n",
        "#cav_tensor = km.cluster_centers_[0]  # cluster centroid (numpy array)\n",
        "# ^^^==== EXERCISE: Try out different CAVs (supervised and unsupervised)\n",
        "\n",
        "cav_tensor = torch.tensor(cav_tensor).to(device)\n",
        "\n",
        "# Possibly fix the class to calculate the TCAV score for\n",
        "# (if None, will always calculate the contribution to the predicted class)\n",
        "target_class_idx = None #trainset.class_to_idx['dog']\n",
        "#                  ^^^==== EXERCISE: Try out different target classes\n",
        "\n",
        "\n",
        "# Select test data\n",
        "num_imgs = 80\n",
        "target_imgs = imgs_all[400:400 + num_imgs].to(device)\n",
        "\n",
        "\n",
        "# To collect activations, we again use the pytorch hook mechanism (cf. Grad-CAM).\n",
        "# This time, we retain the gradient.\n",
        "# (1) define the hook functions:\n",
        "activations = None\n",
        "def save_activation_retain_grad(module, input, output):\n",
        "    global activations\n",
        "    activations = output\n",
        "    output.retain_grad()\n",
        "\n",
        "# (2) register the hook\n",
        "target_module = dict(model.named_modules())[last_conv_layer_name]\n",
        "handle_a = target_module.register_forward_hook(save_activation_retain_grad)\n",
        "\n",
        "\n",
        "# Count samples where the CAV positively contributes to the target class\n",
        "positive_count = 0\n",
        "model.eval()\n",
        "for x in tqdm(target_imgs, desc=f\"Images processed for TCAV calculation \"\n",
        "                                f\"(target class {'\\''+classes[target_class_idx]+'\\'' if target_class_idx is not None else 'best'}')\"):\n",
        "\n",
        "    # forward pass\n",
        "    out = model(x.unsqueeze(0))\n",
        "    # choose top predicted class as target\n",
        "    tclass = out.argmax(dim=1).item() if target_class_idx == None else target_class_idx\n",
        "    score = out[0, tclass]\n",
        "\n",
        "    # backward pass: compute gradient of score w.r.t activation\n",
        "    model.zero_grad()\n",
        "    score.backward(retain_graph=False)\n",
        "    act_grad = activations.grad  # [1, C, H, W]\n",
        "\n",
        "    # !! to reduce computational cost !! => used pooled activations\n",
        "    pooled_grad = act_grad.mean(dim=(2,3)).squeeze(0)  # [C]\n",
        "    # directional derivative approx = pooled_grad dot cav\n",
        "    dd = (pooled_grad * cav_tensor).sum().item()\n",
        "\n",
        "    # add to the TCAV score\n",
        "    if dd > 0:\n",
        "        positive_count += 1\n",
        "handle_a.remove()\n",
        "\n",
        "tcav_score = positive_count / num_imgs\n",
        "print('TCAV score (fraction of images with positive directional derivative):', tcav_score)"
      ],
      "metadata": {
        "id": "1bPm5ddQ_Ddb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 7) Wrap up\n",
        "\n",
        "Some thoughts for further discussion:\n",
        "\n",
        "* Compare Grad‑CAM explanations to concept explanations (visualized prototypes and TCAV scores). What are the advantages and limitations of each?\n",
        "* Compare unsupervised and supervised results regarding the concept-to-output sensitivity. What are differences in interpretability? How would you quantify this?\n",
        "* How would you improve concept datasets for a real analysis (human labeling, multiple concept examples, negatives, etc.)?\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PAxTDCeO_l0L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKla6PMxBNI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}